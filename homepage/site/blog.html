<!doctype html>
<html lang="en">
  <head>
    <title>James Massucco | Blog</title>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="icon" type="image/png" href="assets/icon/favicon-96x96.png" sizes="96x96" />
    <link rel="shortcut icon" href="assets/icon/favicon.ico" />
    <link rel="stylesheet" href="styles/main.css" />
    <link rel="stylesheet" href="styles/blog.css" />
  </head>
  <body>
    <a href="/" class="site-home-link">
      <img src="/assets/icon/favicon-96x96.png" alt="Logo" class="site-logo" />
      Home
    </a>

    <h1>Blog</h1>
    <section>
      <div class="blog-stack">
        <div class="card" href="#">
          <div class="content">
            <h1>A Real Website</h1>
            <p>4/21/2025</p>
            <p>
              Now that I had something to host (Grafana with metrics for my server), I wanted to
              have a real web address where anyone could access it. I looked at a few different web
              domain lease platforms, but ultimately recognized Cloudflare as the best choice
              because of their great offering of services for DNS routing as well as the wide array
              of protections for DOS attacks and similar. I purchased
              <code>jamesmassucco.com</code> for $20/yr.
            </p>
            <p>
              Before allowing traffic to route to my server, I wanted to make sure I understood the
              security implications, and wanted to ensure that I did everything I could to keep it
              secure and not put my home network at risk of hacking. I found a great
              <a
                href="https://www.reddit.com/r/webhosting/comments/as8f6q/comment/egskkmy/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button"
                >Reddit comment</a
              >
              with a lot of the details I needed. The basic steps were:
            </p>
            <ol>
              <li>
                Tell Cloudflare to route traffic to your website to your public home IP address
              </li>
              <li>
                Tell Cloudflare to proxy your IP address, so that anyone accessing your site doesn't
                see your actual home IP address
              </li>
              <li>Tell your server to ONLY accept traffic from Cloudlfare IP addresses</li>
            </ol>
            <p>
              To accomplish points 1 and 2, I configured them via Cloudflare's website. I also
              realized that my home IP address could change, so I added
              <a href="https://hub.docker.com/r/favonia/cloudflare-ddns">cloudflare-ddns</a> to my
              setup - this tool automatically checks your public IP address on a regular cadence and
              then uses a Cloudlfare API token you provide it to tell Cloudflare your new IP address
              if it has changed.
            </p>
            <p>
              To accomplish point 3, I needed a reverse proxy. A reverse proxy is a lot like a
              router, in that it directs traffic in both directions based on specific allowed/known
              routes. A reverse proxy can be configured to receive all the traffic hitting your
              server, and then direct (or block) that traffic to internal services (e.g. Grafana,
              your website, etc.). I had heard of <code>nginx</code> before but decided that
              <code>traefik</code> seemed simpler and better suited for my basic use case. With the
              help of ChatGPT (yes, I get a lot of help from ChatGPT), I set up a basic
              docker-compose for <code>traefik</code> and told it to only accept traffic from the
              Cloudflare IP addresses. Since I wanted to be extra, I also wrote a script that would
              read the Cloudflare IP addresses using <code>curl</code> and then write them to an
              <code>.env</code> file used by <code>traefik</code> so that if they ever change (which
              they almost never do), I could easily update them. Automation for the sake of
              automation? Probably.
            </p>
            <p>
              Once I had <code>traefik</code> set up, I added labels to the docker-compose entry for
              Grafana that enabled it as a routable service for <code>traefik</code> and then
              registered <code>traefik</code> routers for <code>http</code> and
              <code>https</code> traffic to Grafana. I originally only did <code>https</code>, but
              couldn't get that to work and during debug found that if I added an
              <code>http</code> router as well, it would work... I didn't figure out yet why that's
              required, so for now I just left it in. I registered a sub-domain
              (<code>grafana.jamesmassucco.com</code>) in Cloudflare and used that as the router
              source for Grafana traffic, and boom -
              <a href="https://grafana.jamesmassucco.com/">grafana.jamesmassucco.com</a> was live!
            </p>
            <h3>Note on Docker Deployments</h3>
            <p>
              At this point, I decided I had too much going on to just have a single
              <code>docker-compose.yml</code> file, so I decided to a bit of housekeeping
              organization. I setup a <code>networking/</code> directory for
              <code>traefik</code> and <code>cloudflare-ddns</code>, and a
              <code>monitoring/</code> directory for Grafana and Prometheus. Each directory got a
              separate <code>docker-compose.yml</code> file and it's own
              <code>start.sh</code> script. The start script is really a "restart" script, because
              it does the following things:
            </p>
            <ol>
              <li>Change to current director (<code>cd "$(dirname "$0")"</code>)</li>
              <li>Shut down containers (<code>sudo docker compose down</code>)</li>
              <li>
                Update container images from the docker repo (<code>sudo docker compose pull</code>)
              </li>
              <li>Start up containers (<code>sudo docker compose up -d</code>)</li>
            </ol>
            <p>
              The <code>docker compose</code> command only looks at containers managed by the local
              <code>docker-compose.yml</code> file, so by having separate
              <code>start.sh</code> scripts in each directory, I can separately restart the services
              relevant to that aspect of the server. I also set up a top-level
              <code>start_all.sh</code> that just calls each of the <code>start.sh</code> scripts
              from the subdirectories, so I can easily redeploy all of the services if needed.
            </p>
            <p>
              Next up is a deeper dive into security to make triple sure I'm doing everything I can
              to prevent getting hacked!
            </p>
          </div>
        </div>

        <div class="card" href="#">
          <div class="content">
            <h1>Provisioning Server + Metrics Monitoring</h1>
            <p>4/20/25</p>
            <h2>Hardware</h2>
            <p>
              To start the project, I wanted a cheap mini-PC to experiment with. I chose the
              <a href="https://www.amazon.com/dp/B0DQBMRTJ2">Beelink S13</a> because it was on sale
              ($200) and has reasonable specs (Intel N150, 16GB RAM, 1TB SSD). The Beelink brand
              seemed reasonably well-liked based on reviews and some listicles I checked out. In any
              case, my intention is to add more compute nodes later to form a cluster, so all I
              really needed was something I could get started with.
            </p>
            <h2>Provisioning</h2>
            <p>
              When I received the server, it had Windows 11 on it. Windows is... not my favorite
              environment to work in, and not that well-suited to software development, so I decided
              to install Ubuntu Server instead. Specifically, I went with Ubuntu Server 22.04.2, the
              latest Long-Term Support (LTS) version available. I followed this
              <a href="https://ubuntu.com/tutorials/create-a-usb-stick-on-macos">guide</a> to create
              a bootable USB stick, plugged it into the Beelink, and rebooted while holding down a
              specific key on the keyboard that let me choose what drive too boot from, and then
              after a few minutes of playing "answer the prompts", I had Ubuntu Server running!
            </p>
            <p>
              My first goal was to be able to remotely adminster the server from my laptop over SSH,
              so I set about figuring out the network setup. I plugged the Beelink into my router
              but couldn't get a network connection, until I realized I needed to activate the
              ethernet interface. Turns out these servers require a lot of explicit instruction on
              what you want them to do! Once I got the port enabled, I was able to do a basic
              connectivity test (<code>ping 8.8.8.8</code>, which is Google's DNS server). After
              that, I had to install ssh and then allow ssh access on port 22. While I was doing
              this, I went down a tangent reading Ubuntu's
              <a
                href="https://documentation.ubuntu.com/server/explanation/intro-to/security/index.html"
                >guide to security</a
              >
              and got a little more familiar with the Universal Firewall (<code>ufw</code>). After I
              satisfied my curiousity, I checked the server's IP address with <code>ip a</code> and
              then on my laptop I ran <code>ssh jmassucco@&lt;ip_address&gt;</code> and I was in! To
              make things a little easier, I assigned the Beelink a hostname
              (<code>ubuntu-server-1</code>) and after a little experimenting, figured that I could
              access that from my laptop at <code>ssh jmassucco@ubuntu-server-1.local</code>. Now
              that I could reliably access the server, I disconnected it from my monitor and stowed
              it in a nondescript back corner of my desk.
            </p>
            <h2>Monitoring</h2>
            <p>
              The first real project I wanted to do was to setup self-health monitoring of the
              server. I was already familiar with Prometheus (at least in concept) and have used
              Grafana quite a bit, so decided to go with that combo. I knew from the beginning that
              I wanted to use Docker to manage containers for each of the services running on my
              server, so with the help of ChatGPT, I got a simple docker-compose file written to
              deploy Prometheus and node-exporter.
            </p>
            <p>
              Prometheus is basically a data scraper and database system which is optimized for
              collecting metrics. So, by itself it doesn't measure anything. But the Prometheus
              project provides a service called <code>node-exporter</code> which records standard
              metrics like CPU and RAM usage and makes them available to Prometheus. So with the two
              of those deployed together, I was able to view metrics on my cluster in Prometheus at
              http://ubuntu-server-1.local.:9090/. But I wanted nice plots that I could save and
              view together, and for that I needed Grafana.
            </p>
            <p>
              Grafana is a data visualization tool that can be connected to any number of backing
              databases. You can then write queries to those databases and save them in Grafana
              Panels. Put a bunch of panels together, and you've got a Dashboard which provides a
              fixed set of queries plotted as graphs to visualize a wide array of data in a simple
              and familiar view. You can change the time range and the panels automatically query
              the database(s) for the right data, and you can also set auto-refresh so that you can
              view live data as it comes in. Grafana also has a huge library of publically saved
              dashboards that can be easily imported to your instance, so it was a total breeze to
              set up Grafana (docker-compose by ChatGPT) and then import a few public dashboards
              built for Prometheus + node-exporter and pick my favorite one, and I was viewing live
              metrics on my cluster in beautiful graph plots!
            </p>
            <p>
              After I got the metrics set up, I realized I also wanted to see metrics oriented
              around my Docker containers, and I found the Google open-source cadvisor (short for
              Container Advisor) that aggregates docker container resource utilization stats and
              makes them available to Prometheus. After a quick docker-compose update and redeploy,
              and another public dashboard import to Grafana, I was viewing these metrics as well!
            </p>
            <p>
              At this point, I was very happy with my setup. I decided that my next step was to make
              what I'd built so far available on a public website (jamesmassucco.com). But I was out
              of daylight, so decided it was a project for another day.
            </p>
          </div>
        </div>

        <div class="card" href="#">
          <div class="content">
            <h1>Starting a homelab</h1>
            <p>4/19/25</p>
            <p>
              This is the first of (hopefully) many posts I'll make to document the process of
              building my home server! I started this project because I want to be able to work on
              fun software infrastructure things without it always having to be for work. I've
              historically been very much a workaholic, and though I'm an Electrical Engineer by
              schooling, I am lately working on more and more software and less and less hardware.
              My girlfriend recently quoted a coworker as saying "hobbies... I've always meant to
              get some of those" and it was a bit of a wake up call for me. So here I am, getting a
              hobby!
            </p>
            <p>
              My primary goal with this project, as inspired by
              <a href="https://www.reddit.com/r/homelab/wiki/introduction/">r/homelab</a>, is to
              learn! I'm also interested in reinforcing my skills in software, in case I pursue an
              even more software-focused career path in the future.
            </p>
            <p>I'm starting this project with a few learning goals:</p>
            <ol>
              <li>Basic network and security, so I don't get my home network hacked</li>
              <li>Domains and DNS, so I can host a website from my home server</li>
              <li>Docker, because it seems useful</li>
              <li>
                Kubernetes, so I can build a cheap cluster of hardware (and also because it seems
                like a "hot" thing to know for SW infra jobs)
              </li>
              <li>
                HTML/CSS (and more advanced web stuff) so that I can build some cool webapps
                (purpose TBD other than that I like making stuff)
              </li>
              <li>LLMs, so I can make my webapps "powered by AI" lol</li>
            </ol>
            <p>
              I'm receiving my (first) server today, and will post details of my learning and setup
              as soon as I get started!
            </p>
          </div>
        </div>
      </div>
    </section>

    <footer>© 2025 James Massucco</footer>
  </body>
</html>
